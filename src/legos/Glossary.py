""" #TODO automate this
        GlossaryIndex = {
        "gradient": Glossary_Blame_Signal,
        "epoch": Glossary_Epoch,
        "sgd": Glossary_SGD,
        "loss": Glossary_LossFunction,
        "mse": Glossary_MSE,
        "activation": Glossary_Activation,
        "relu": Glossary_ReLU,
        "softmax": Glossary_Softmax,
        "weight_init": Glossary_WeightInit,
        "learning_rate": Glossary_LearningRate,
        "backpropagation": Glossary_Backprop,
        "activation": Glossary_ActivationOut,
        "input_activation": Glossary_ActivationIn,
    }
"""

 #Let's Rename the Popup Fields (Bottom-Right Quadrant)
"""
Current Label	Proposed New Label	Why It Works
Receiving Blame	Accusation	Clear, emotional, intuitive â€” makes you question your influence
Itâ€™s Blame Ã— MyResp	My Share	Like child says, "Youâ€™re 40% responsible"
My Blame from All	Total Accusation	Could keep MBFA label, just rephrased
Acop Blame = MBFA Ã— AG	Accepted Blame	Final amount I admit was my fault

"""

class Glossary:
    """
    ğŸ‘ï¸ We think in languageâ€¦ and if that language is not clear, neither is our thinking.
    ğŸ§  This defines alternative terms that are more accurate, explainable, and humane.
    ğŸ¥‚ Each entry replaces a misleading legacy term with a transparent, honest one.


    temp list for window...
    Spreading Blame from Neuron 20 to Neuron 10
    Influence.  How much of Neuron 20â€™s output was caused by Neuron 10?"
    My Share:   Based on influence how much actual blame is received.
    MBFA:       My blame from all.
    Accepted:   MBFA * Act Grad



    """
    def __init__(self, old_name, new_name, why_old_name_sucks="Because", definition="Please add"):
        self.old_name = old_name
        self.new_name = new_name
        self.why_old_name_sucks = why_old_name_sucks
        self.definition = definition



    def __repr__(self):
        return f"Glossary Entry: '{self.old_name}' â†’ '{self.new_name}'"

Glossary_RawAdjustment = Glossary(
    old_name="Gradient",
    new_name="Raw Adjustment",
    why_old_name_sucks="""
        'Gradient' is a borrowed metaphor from calculus that implies slopes, geometry, and smooth manifolds â€”
        concepts that donâ€™t apply cleanly in the high-dimensional, discrete world of neural networks.
        Worse, it masks what the value *actually is*: a direct precursor to a weight change.
    """,
    definition="""
        The Raw Adjustment is the value that tells a weight how it should change to reduce error â€”
        before it's scaled by the learning rate.

        It is calculated per weight during backpropagation:
            Raw Adjustment = Input Ã— Accepted Blame

        The learning rate turns this into the Final Adjustment:
            Final Adjustment = Learning Rate Ã— Raw Adjustment
    """
)

Glossary_Accepted_Blame = Glossary(
    old_name="Error Signal",
    new_name="Accepted Blame",
    why_old_name_sucks=(
        "'Error Signal' is vague and sounds like a generic alert. "
        "It doesn't capture the idea that the neuron is *receiving* responsibility from downstream."
    ),
    definition="""
        Accepted Blame is the total amount of responsibility a neuron inherits from the next layer during backpropagation.
        It tells the neuron: "Based on how you influenced the output, this much of the total mistake is yours."
        
        It is calculated by summing up the blame passed backward from all connected neurons in the next layer.
        This value is then used (along with the activation function's gradient) to determine how each incoming weight should be adjusted.
    """
)

Glossary_Accusation = Glossary(
    old_name="There is no term for this",
    new_name="Accusation",
    why_old_name_sucks="""
        Each child neuron (on the next layer) sends a portion of its blame back to the current neuron.

        That portion is: Childâ€™s BlameÃ—Weight that connects us
        That weight is the one from current neuron â†’ child neuron
        So in childâ€™s frame, itâ€™s: "Hereâ€™s how much you influenced me, scaled by how much Iâ€™m hurting."

        ğŸ§  Traditional Term?  In backpropagation literature, this is usually unnamed â€” itâ€™s just a step inside the chain rule.
            But if you do find a name, it might show up as:
            â€œWeighted errorâ€ (but vague and overloaded)
            â€œBackpropagated gradientâ€ (which is just... nonsense for clarity)
            Or most often: nothing. It's treated as an anonymous middle step.
        """,
    definition="""
        An Accusation is an incoming blame value passed from a neuron in the next layer (a child) to this neuron.

        Each child neuron says:  
        â€œPart of my error is your fault â€” hereâ€™s how much you influenced me.â€

        Accusation is calculated as:
        â†’ `BlameFromChild Ã— InfluenceFromThisNeuron`

        Multiple accusations can be received from multiple children.
        These are summed into **My Blame From All (MBFA)** â€” the total raw blame this neuron is being assigned.
        """
)

Glossary_BatchTotal = Glossary(
    old_name="N/A (no consistent term for this in traditional ML)",
    new_name="Batch Total",
    why_old_name_sucks="""
        There isnâ€™t even a standard name for this â€” which is wild, considering how critical it is.
        Most frameworks quietly accumulate gradients across a batch without ever surfacing them.
        That hides the connection between per-sample adjustments and the final weight change.
        """,
    definition="""
        Batch Total is the **sum of raw adjustments across all samples in the current batch** for a given weight.

        It answers the question:
        â€œAcross this entire batch, how much are we being told to adjust this weight?â€

        Raw Adjustment = Input Ã— Accepted Blame  
        Batch Total    = Î£ Raw Adjustments (one per sample)

        This value is what ultimately gets scaled by the learning rate to produce the final weight update.

        â†’ Final Adjustment = Batch Total Ã— Learning Rate

        NeuroForge shows this explicitly to maintain full auditability in batch training modes â€” no math hidden behind the curtain.
        """
)



Glossary_ConnectionWeight = Glossary(
    old_name="Weight between neurons",
    new_name="Connection Weight",
    why_old_name_sucks="""
        'Weight' is vague and generic. It doesn't clarify *whose* weight it is, *what direction* it connects, or *why it matters* in both forward and backward flow.
        Most explanations treat weights as static scalars rather than the **dual-purpose conduits** they truly are.
        """,
    definition="""
        A Connection Weight links two neurons: one in the current layer (the sender) and one in the next layer (the receiver).

        It has two critical roles:

        ğŸ”¹ **Forward Pass (Influence)**:  
        The weight scales the sender neuron's output â€” determining how much it contributes to the receiver's input.  
        â†’ `Output Ã— Weight â†’ Raw Sum of receiver`

        ğŸ”¹ **Backward Pass (Accusation Pathway)**:  
        The same weight also determines how much of the receiverâ€™s blame is sent backward to the sender.  
        â†’ `Accepted Blame of receiver Ã— Weight â†’ Accusation sent back to sender`

        Connection Weights are the exact same values used in both directions of learning.  
        - In forward pass, they **control influence**.  
        - In backprop, they **carry blame**.

        NeuroForge tracks and adjusts these weights during training via:
        â†’ `Adjustment = Raw Adjustment Ã— Learning Rate`
        """
)



Glossary_Parent = Glossary(
    old_name="Previous layer neurons",
    new_name="Parents",
    why_old_name_sucks="Itâ€™s not wrong, but itâ€™s sterile. Parents *feed* the current neuron, just like in a family tree.",
    definition="Neurons from the layer to the left whose outputs serve as inputs to the current neuron. They â€˜parentâ€™ the activation."
)

Glossary_Parent2 = Glossary(
    old_name="Next layer neurons",
    new_name="Parents (Blame-Centric)",
    why_old_name_sucks="The structural view calls these â€˜children,â€™ but in backprop, they assign blame â€” like a parent scolding their kid.",
    definition="""
        Neurons in the layer to the right (next layer) that receive this neuronâ€™s output.
        But during backpropagation, they become the â€˜parentsâ€™ â€” evaluating the decision and assigning blame backward.
        """
)


Glossary_Child = Glossary(
    old_name="Next layer neurons",
    new_name="Children",
    why_old_name_sucks="Again, technically fine, but vague. These are the neurons who receive the current neuronâ€™s output. Children inherit activations.",
    definition="Neurons in the layer to the right who receive the output of the current neuron. If this neuron fires, theyâ€™re the ones who react to it."
)

Glossary_Child2 = Glossary(
    old_name="Previous layer neurons",
    new_name="Children (Blame-Centric)",
    why_old_name_sucks="Although they come earlier in structure, these neurons are like kids â€” their outputs shaped the current neuronâ€™s mistake.",
    definition="""
        Neurons in the previous layer who sent inputs to this neuron.
        During backpropagation, this neuron pushes blame onto them â€” just like parents blaming their kids for a bad family decision.
        """
)

Glossary_Sibling = Glossary(
    old_name="Neurons in same layer",
    new_name="Siblings",
    why_old_name_sucks="â€˜Same layerâ€™ is visually descriptive but not intuitive. Siblings process in parallel â€” same inputs, different roles.",
    definition="Other neurons in the same layer â€” they receive the same inputs but have different weights, biases, and behaviors."
)

Glossary_Activation = Glossary(
    old_name="Activation",
    new_name="Output",
    why_old_name_sucks="""
        'Activation' is a biology metaphor gone rogue.
        It implies something binary or mysterious. But in neural networks, itâ€™s just the output value of a neuron â€” the signal it sends forward.
        """,
    definition="""
        The 'activation' is simply the result a neuron sends to the next layer.
        After summing its inputs and applying a transformation (like ReLU), the neuron outputs a number.
        That number becomes input to the next layer.
        So letâ€™s call it what it is: the neuronâ€™s output.
        """
)


Glossary_ActivationIn = Glossary(
    old_name="Input Activation",
    new_name="Input",
    why_old_name_sucks="""
        Stacking 'input' and 'activation' adds noise to a clean idea.
        It's just the number this neuron receives â€” from its parents.
        Calling it an 'activation' before itâ€™s even been activated makes no sense.
        """,
    definition="""
        This is the value a neuron receives from another neuron in the previous layer.
        You might have 3 parents, each sending you a number â€” those are your inputs.
        After combining them, you decide what to output.
        """
)


Glossary_Epoch = Glossary(
    old_name="Epoch",
    new_name="Full Data Pass",
    why_old_name_sucks="The term 'epoch' is inherited from timekeeping and means nothing to most learners. It hides what actually happens: one complete loop over all training data.",
    definition="""
        An epoch is a full pass through the entire training dataset.
        Every sample is shown to the model exactly once.
        We often repeat this multiple times to improve learning.
        """
)

Glossary_SGD = Glossary(
    old_name="Stochastic Gradient Descent",
    new_name="Random Update Trick",
    why_old_name_sucks="""
        It's not stochastic if you're using a full batch.
        Itâ€™s not gradient descent in any continuous sense.
        And it hides the fact that the only reason it works is because randomness sometimes helps.
        """,
    definition="""
        This strategy updates weights based on one or a few samples at a time.
        It adds noise and chaos to the learning process, which sometimes prevents bad patterns from forming.
        But itâ€™s less about theory, and more about getting lucky with random steps.
        """
)


Glossary_LossFunction = Glossary(
    old_name="Loss Function",
    new_name="Gradient Source",
    why_old_name_sucks="""
        The term 'loss' implies the value matters â€” it doesnâ€™t.
        Only the derivative of the loss (i.e., the gradient) affects training.
        Models donâ€™t â€˜lookâ€™ at loss, they respond to the signal behind it.
        """,
    definition="""
        A gradient source tells the network how to respond to its mistake.
        It doesn't matter how big the loss number is â€” only which direction to nudge each weight.
        Loss functions are like signposts. You ignore the post. You follow the arrow.
        """
)


Glossary_MSE = Glossary(
    old_name="Mean Squared Error",
    new_name="Squared Error Signal",
    why_old_name_sucks="""
        MSE is not really about squaring anything for training â€” the model only sees 2 * error.
        The square is for humans comparing models. For the model, itâ€™s just another way to blame.
        """,
    definition="""
        The gradient of MSE is just 2 times the error.
        The â€˜squaredâ€™ part makes bigger mistakes count more â€” thatâ€™s all.
        Itâ€™s not more precise, just more punishing.
        """
)


Glossary_Activation = Glossary(
    old_name="Activation Function",
    new_name="Output Transformer",
    why_old_name_sucks="""
        'Activation' makes it sound biological and binary â€” like a neuron turns on or off.
        But in most networks, these functions just reshape the neuronâ€™s output.
        """,
    definition="""
        This function transforms the output of a neuron.
        It can compress it (like sigmoid), sharpen it (like ReLU), or center it (like tanh).
        Itâ€™s not turning anything â€˜onâ€™ â€” itâ€™s just shaping how signals flow.
        """
)


Glossary_ReLU = Glossary(
    old_name="ReLU (Rectified Linear Unit)",
    new_name="Signal Gate",
    why_old_name_sucks="""
        The acronym 'ReLU' is meaningless to beginners, and 'unit' makes it sound like a separate entity.
        There's no 'rectifier' or 'unit' â€” just a rule that blocks negative numbers.
        """,
    definition="""
        ReLU lets positive values through and blocks negative ones.
        It's like a gate: if the signal is positive, pass it; if not, shut it down.
        Most networks use ReLU to force neurons to contribute *only when excited.*
        """
)


Glossary_Softmax = Glossary(
    old_name="Softmax",
    new_name="Probability Estimator",
    why_old_name_sucks="""
        'Softmax' sounds technical but tells you nothing.
        It doesnâ€™t soften anything or maximize anything in human terms.
        It just squishes numbers into a probability distribution.
        """,
    definition="""
        Takes a list of values and turns them into a set of probabilities that add up to 1.
        Higher numbers become more likely â€” but everything still gets some chance.
        Used to make the model â€˜chooseâ€™ one of many possible outcomes.
        """
)


Glossary_WeightInit = Glossary(
    old_name="Weight Initialization",
    new_name="Start Guess",
    why_old_name_sucks="""
        'Initialization' sounds technical and forgettable.
        But your modelâ€™s *entire future* is shaped by how it starts.
        This is its first guess â€” and a bad first guess can sabotage everything.
        """,
    definition="""
        Before training starts, every weight needs a starting value.
        That starting value can be random, balanced, zeroed, or carefully scaled.
        Itâ€™s the modelâ€™s first take â€” and it matters more than most people realize.
        """
)


Glossary_LearningRate = Glossary(
    old_name="Learning Rate",
    new_name="Nudge Size",
    why_old_name_sucks="""
        Itâ€™s not a rate. Itâ€™s not about learning. Itâ€™s a multiplier on blame.
        â€˜Learning rateâ€™ makes it sound like intelligence. Itâ€™s just how big the updates are.
        """,
    definition="""
        This is how strongly the model reacts to blame.
        A big nudge makes it learn fast but risks overshooting.
        A small nudge is stable but can be slow or stuck.
        Tuning this is like finding the right volume for a microphone: too low = nothing, too high = feedback.
        """
)


Glossary_Backprop = Glossary(
    old_name="Backpropagation",
    new_name="Blame Relay",
    why_old_name_sucks="""
        'Backpropagation' sounds like a quantum physics term.
        It hides the simple truth: blame is passed backward through the network to assign responsibility.
        """,
    definition="""
        After making a prediction, the network looks at how wrong it was.
        It then passes that blame backward through each layer,
        so every neuron knows how much it contributed to the error.
        This is how learning happens: every weight gets its share of the blame.
        """
)


"""
ğŸ¬ The Family Movie Metaphor
Letâ€™s break it down in your words â€” and fill in the labels as we go:

ğŸ¥ Forward Prop = Decision Phase
Children (inputs) toss out ideas:
"Letâ€™s watch a Marvel movie!"
"How about something animated?"

Parents (middle layer) weigh the suggestions:
"Hmm... maybe Spider-Verse? Or Elemental?"
They combine the kidsâ€™ preferences with their own biases (weights), then...

Grandparents (output layer) make the final call:
"Alright, weâ€™ll go with Spider-Verse."

The signal has moved forward through the family tree â€” each generation processing what they received.

ğŸ” Backward Prop = Blame Phase
They watch the movie... and itâ€™s a disaster. ğŸ˜©
("Too loud!" "Didnâ€™t understand it!"_)

Grandparents say:
"Why did we agree to this? The parents convinced us!"

Parents go:
"We were just trying to make the kids happy!"

And the blame flows backward â€” each layer passing responsibility upstream.

ğŸ§  What This Solves
âœ… Preserves parentâ€“child direction in forward flow (still accurate structurally)

âœ… Justifies blame flowing in reverse during backprop (natural and intuitive)

âœ… Makes adjustments feel like â€œfamily therapyâ€ after a bad choice ğŸ˜„

âœ… Reinforces the core idea:
"Outputs are built by children â†’ parents â†’ grandparents, but errors are explained by grandparents â†’ parents â†’ children."

ğŸ’¡ Tooltip/Wiki Version:
Forward Pass
â€œThe family decides which movie to watch â€” kids suggest, parents refine, grandparents decide.â€

Backward Pass
â€œThe movie sucked. Grandparents blame parents. Parents blame kids. Everyone adjusts their preferences next time.â€

"""