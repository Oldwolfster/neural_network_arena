""" #TODO automate this
        GlossaryIndex = {
        "gradient": Glossary_Blame_Signal,
        "epoch": Glossary_Epoch,
        "sgd": Glossary_SGD,
        "loss": Glossary_LossFunction,
        "mse": Glossary_MSE,
        "activation": Glossary_Activation,
        "relu": Glossary_ReLU,
        "softmax": Glossary_Softmax,
        "weight_init": Glossary_WeightInit,
        "learning_rate": Glossary_LearningRate,
        "backpropagation": Glossary_Backprop,
        "activation": Glossary_ActivationOut,
        "input_activation": Glossary_ActivationIn,
    }
"""

 #Let's Rename the Popup Fields (Bottom-Right Quadrant)
"""
Current Label	Proposed New Label	Why It Works
Receiving Blame	Accusation	Clear, emotional, intuitive ‚Äî makes you question your influence
It‚Äôs Blame √ó MyResp	My Share	Like child says, "You‚Äôre 40% responsible"
My Blame from All	Total Accusation	Could keep MBFA label, just rephrased
Acop Blame = MBFA √ó AG	Accepted Blame	Final amount I admit was my fault

"""

class Glossary:
    """
    üëÅÔ∏è We think in language‚Ä¶ and if that language is not clear, neither is our thinking.
    üß† This defines alternative terms that are more accurate, explainable, and humane.
    ü•Ç Each entry replaces a misleading legacy term with a transparent, honest one.


    temp list for window...
    Spreading Blame from Neuron 20 to Neuron 10
    Influence.  How much of Neuron 20‚Äôs output was caused by Neuron 10?"
    My Share:   Based on influence how much actual blame is received.
    MBFA:       My blame from all.
    Accepted:   MBFA * Act Grad



    """
    def __init__(self, old_name, new_name, why_old_name_sucks="Because", definition="Please add"):
        self.old_name = old_name
        self.new_name = new_name
        self.why_old_name_sucks = why_old_name_sucks
        self.definition = definition



    def __repr__(self):
        return f"Glossary Entry: '{self.old_name}' ‚Üí '{self.new_name}'"

Glossary_RawAdjustment = Glossary(
    old_name="Gradient",
    new_name="Raw Adjustment",
    why_old_name_sucks="""
        'Gradient' is a borrowed metaphor from calculus that implies slopes, geometry, and smooth manifolds ‚Äî
        concepts that don‚Äôt apply cleanly in the high-dimensional, discrete world of neural networks.
        Worse, it masks what the value *actually is*: a direct precursor to a weight change.
    """,
    definition="""
        The Raw Adjustment is the value that tells a weight how it should change to reduce error ‚Äî
        before it's scaled by the learning rate.

        It is calculated per weight during backpropagation:
            Raw Adjustment = Input √ó Accepted Blame

        The learning rate turns this into the Final Adjustment:
            Final Adjustment = Learning Rate √ó Raw Adjustment
    """
)

Glossary_Accepted_Blame = Glossary(
    old_name="Error Signal",
    new_name="Accepted Blame",
    why_old_name_sucks=(
        "'Error Signal' is vague and sounds like a generic alert. "
        "It doesn't capture the idea that the neuron is *receiving* responsibility from downstream."
    ),
    definition="""
        Accepted Blame is the total amount of responsibility a neuron inherits from the next layer during backpropagation.
        It tells the neuron: "Based on how you influenced the output, this much of the total mistake is yours."
        
        It is calculated by summing up the blame passed backward from all connected neurons in the next layer.
        This value is then used (along with the activation function's gradient) to determine how each incoming weight should be adjusted.
    """
)

Glossary_Accusation = Glossary(
    old_name="There is no term for this",
    new_name="Accusation",
    why_old_name_sucks="""
        Each child neuron (on the next layer) sends a portion of its blame back to the current neuron.

        That portion is: Child‚Äôs Blame√óWeight that connects us
        That weight is the one from current neuron ‚Üí child neuron
        So in child‚Äôs frame, it‚Äôs: "Here‚Äôs how much you influenced me, scaled by how much I‚Äôm hurting."

        üß† Traditional Term?  In backpropagation literature, this is usually unnamed ‚Äî it‚Äôs just a step inside the chain rule.
            But if you do find a name, it might show up as:
            ‚ÄúWeighted error‚Äù (but vague and overloaded)
            ‚ÄúBackpropagated gradient‚Äù (which is just... nonsense for clarity)
            Or most often: nothing. It's treated as an anonymous middle step.
        """,
    definition="""
        An Accusation is an incoming blame value passed from a neuron in the next layer (a child) to this neuron.

        Each child neuron says:  
        ‚ÄúPart of my error is your fault ‚Äî here‚Äôs how much you influenced me.‚Äù

        Accusation is calculated as:
        ‚Üí `BlameFromChild √ó InfluenceFromThisNeuron`

        Multiple accusations can be received from multiple children.
        These are summed into **My Blame From All (MBFA)** ‚Äî the total raw blame this neuron is being assigned.
        """
)

Glossary_BatchTotal = Glossary(
    old_name="N/A (no consistent term for this in traditional ML)",
    new_name="Batch Total",
    why_old_name_sucks="""
        There isn‚Äôt even a standard name for this ‚Äî which is wild, considering how critical it is.
        Most frameworks quietly accumulate gradients across a batch without ever surfacing them.
        That hides the connection between per-sample adjustments and the final weight change.
        """,
    definition="""
        Batch Total is the **sum of raw adjustments across all samples in the current batch** for a given weight.

        It answers the question:
        ‚ÄúAcross this entire batch, how much are we being told to adjust this weight?‚Äù

        Raw Adjustment = Input √ó Accepted Blame  
        Batch Total    = Œ£ Raw Adjustments (one per sample)

        This value is what ultimately gets scaled by the learning rate to produce the final weight update.

        ‚Üí Final Adjustment = Batch Total √ó Learning Rate

        NeuroForge shows this explicitly to maintain full auditability in batch training modes ‚Äî no math hidden behind the curtain.
        """
)



Glossary_ConnectionWeight = Glossary(
    old_name="Weight between neurons",
    new_name="Connection Weight",
    why_old_name_sucks="""
        'Weight' is vague and generic. It doesn't clarify *whose* weight it is, *what direction* it connects, or *why it matters* in both forward and backward flow.
        Most explanations treat weights as static scalars rather than the **dual-purpose conduits** they truly are.
        """,
    definition="""
        A Connection Weight links two neurons: one in the current layer (the sender) and one in the next layer (the receiver).

        It has two critical roles:

        üîπ **Forward Pass (Influence)**:  
        The weight scales the sender neuron's output ‚Äî determining how much it contributes to the receiver's input.  
        ‚Üí `Output √ó Weight ‚Üí Raw Sum of receiver`

        üîπ **Backward Pass (Accusation Pathway)**:  
        The same weight also determines how much of the receiver‚Äôs blame is sent backward to the sender.  
        ‚Üí `Accepted Blame of receiver √ó Weight ‚Üí Accusation sent back to sender`

        Connection Weights are the exact same values used in both directions of learning.  
        - In forward pass, they **control influence**.  
        - In backprop, they **carry blame**.

        NeuroForge tracks and adjusts these weights during training via:
        ‚Üí `Adjustment = Raw Adjustment √ó Learning Rate`
        """
)



Glossary_Parent = Glossary(
    old_name="Previous layer neurons",
    new_name="Parents",
    why_old_name_sucks="It‚Äôs not wrong, but it‚Äôs sterile. Parents *feed* the current neuron, just like in a family tree.",
    definition="Neurons from the layer to the left whose outputs serve as inputs to the current neuron. They ‚Äòparent‚Äô the activation."
)

Glossary_Parent2 = Glossary(
    old_name="Next layer neurons",
    new_name="Parents (Blame-Centric)",
    why_old_name_sucks="The structural view calls these ‚Äòchildren,‚Äô but in backprop, they assign blame ‚Äî like a parent scolding their kid.",
    definition="""
        Neurons in the layer to the right (next layer) that receive this neuron‚Äôs output.
        But during backpropagation, they become the ‚Äòparents‚Äô ‚Äî evaluating the decision and assigning blame backward.
        """
)


Glossary_Child = Glossary(
    old_name="Next layer neurons",
    new_name="Children",
    why_old_name_sucks="Again, technically fine, but vague. These are the neurons who receive the current neuron‚Äôs output. Children inherit activations.",
    definition="Neurons in the layer to the right who receive the output of the current neuron. If this neuron fires, they‚Äôre the ones who react to it."
)

Glossary_Child2 = Glossary(
    old_name="Previous layer neurons",
    new_name="Children (Blame-Centric)",
    why_old_name_sucks="Although they come earlier in structure, these neurons are like kids ‚Äî their outputs shaped the current neuron‚Äôs mistake.",
    definition="""
        Neurons in the previous layer who sent inputs to this neuron.
        During backpropagation, this neuron pushes blame onto them ‚Äî just like parents blaming their kids for a bad family decision.
        """
)

Glossary_Sibling = Glossary(
    old_name="Neurons in same layer",
    new_name="Siblings",
    why_old_name_sucks="‚ÄòSame layer‚Äô is visually descriptive but not intuitive. Siblings process in parallel ‚Äî same inputs, different roles.",
    definition="Other neurons in the same layer ‚Äî they receive the same inputs but have different weights, biases, and behaviors."
)

Glossary_Activation = Glossary(
    old_name="Activation",
    new_name="Output",
    why_old_name_sucks="""
        'Activation' is a biology metaphor gone rogue.
        It implies something binary or mysterious. But in neural networks, it‚Äôs just the output value of a neuron ‚Äî the signal it sends forward.
        """,
    definition="""
        The 'activation' is simply the result a neuron sends to the next layer.
        After summing its inputs and applying a transformation (like ReLU), the neuron outputs a number.
        That number becomes input to the next layer.
        So let‚Äôs call it what it is: the neuron‚Äôs output.
        """
)


Glossary_ActivationIn = Glossary(
    old_name="Input Activation",
    new_name="Input",
    why_old_name_sucks="""
        Stacking 'input' and 'activation' adds noise to a clean idea.
        It's just the number this neuron receives ‚Äî from its parents.
        Calling it an 'activation' before it‚Äôs even been activated makes no sense.
        """,
    definition="""
        This is the value a neuron receives from another neuron in the previous layer.
        You might have 3 parents, each sending you a number ‚Äî those are your inputs.
        After combining them, you decide what to output.
        """
)


Glossary_Epoch = Glossary(
    old_name="Epoch",
    new_name="Full Data Pass",
    why_old_name_sucks="The term 'epoch' is inherited from timekeeping and means nothing to most learners. It hides what actually happens: one complete loop over all training data.",
    definition="""
        An epoch is a full pass through the entire training dataset.
        Every sample is shown to the model exactly once.
        We often repeat this multiple times to improve learning.
        """
)

Glossary_SGD = Glossary(
    old_name="Stochastic Gradient Descent",
    new_name="Random Update Trick",
    why_old_name_sucks="""
        It's not stochastic if you're using a full batch.
        It‚Äôs not gradient descent in any continuous sense.
        And it hides the fact that the only reason it works is because randomness sometimes helps.
        """,
    definition="""
        This strategy updates weights based on one or a few samples at a time.
        It adds noise and chaos to the learning process, which sometimes prevents bad patterns from forming.
        But it‚Äôs less about theory, and more about getting lucky with random steps.
        """
)


Glossary_LossFunction = Glossary(
    old_name="Loss Function",
    new_name="Gradient Source",
    why_old_name_sucks="""
        The term 'loss' implies the value matters ‚Äî it doesn‚Äôt.
        Only the derivative of the loss (i.e., the gradient) affects training.
        Models don‚Äôt ‚Äòlook‚Äô at loss, they respond to the signal behind it.
        """,
    definition="""
        A gradient source tells the network how to respond to its mistake.
        It doesn't matter how big the loss number is ‚Äî only which direction to nudge each weight.
        Loss functions are like signposts. You ignore the post. You follow the arrow.
        """
)


Glossary_MSE = Glossary(
    old_name="Mean Squared Error",
    new_name="Squared Error Signal",
    why_old_name_sucks="""
        MSE is not really about squaring anything for training ‚Äî the model only sees 2 * error.
        The square is for humans comparing models. For the model, it‚Äôs just another way to blame.
        """,
    definition="""
        The gradient of MSE is just 2 times the error.
        The ‚Äòsquared‚Äô part makes bigger mistakes count more ‚Äî that‚Äôs all.
        It‚Äôs not more precise, just more punishing.
        """
)


Glossary_Activation = Glossary(
    old_name="Activation Function",
    new_name="Output Transformer",
    why_old_name_sucks="""
        'Activation' makes it sound biological and binary ‚Äî like a neuron turns on or off.
        But in most networks, these functions just reshape the neuron‚Äôs output.
        """,
    definition="""
        This function transforms the output of a neuron.
        It can compress it (like sigmoid), sharpen it (like ReLU), or center it (like tanh).
        It‚Äôs not turning anything ‚Äòon‚Äô ‚Äî it‚Äôs just shaping how signals flow.
        """
)


Glossary_ReLU = Glossary(
    old_name="ReLU (Rectified Linear Unit)",
    new_name="Signal Gate",
    why_old_name_sucks="""
        The acronym 'ReLU' is meaningless to beginners, and 'unit' makes it sound like a separate entity.
        There's no 'rectifier' or 'unit' ‚Äî just a rule that blocks negative numbers.
        """,
    definition="""
        ReLU lets positive values through and blocks negative ones.
        It's like a gate: if the signal is positive, pass it; if not, shut it down.
        Most networks use ReLU to force neurons to contribute *only when excited.*
        """
)


Glossary_Softmax = Glossary(
    old_name="Softmax",
    new_name="Probability Estimator",
    why_old_name_sucks="""
        'Softmax' sounds technical but tells you nothing.
        It doesn‚Äôt soften anything or maximize anything in human terms.
        It just squishes numbers into a probability distribution.
        """,
    definition="""
        Takes a list of values and turns them into a set of probabilities that add up to 1.
        Higher numbers become more likely ‚Äî but everything still gets some chance.
        Used to make the model ‚Äòchoose‚Äô one of many possible outcomes.
        """
)


Glossary_WeightInit = Glossary(
    old_name="Weight Initialization",
    new_name="Start Guess",
    why_old_name_sucks="""
        'Initialization' sounds technical and forgettable.
        But your model‚Äôs *entire future* is shaped by how it starts.
        This is its first guess ‚Äî and a bad first guess can sabotage everything.
        """,
    definition="""
        Before training starts, every weight needs a starting value.
        That starting value can be random, balanced, zeroed, or carefully scaled.
        It‚Äôs the model‚Äôs first take ‚Äî and it matters more than most people realize.
        """
)


Glossary_LearningRate = Glossary(
    old_name="Learning Rate",
    new_name="Nudge Size",
    why_old_name_sucks="""
        It‚Äôs not a rate. It‚Äôs not about learning. It‚Äôs a multiplier on blame.
        ‚ÄòLearning rate‚Äô makes it sound like intelligence. It‚Äôs just how big the updates are.
        """,
    definition="""
        This is how strongly the model reacts to blame.
        A big nudge makes it learn fast but risks overshooting.
        A small nudge is stable but can be slow or stuck.
        Tuning this is like finding the right volume for a microphone: too low = nothing, too high = feedback.
        """
)


Glossary_Backprop = Glossary(
    old_name="Backpropagation",
    new_name="Blame Relay",
    why_old_name_sucks="""
        'Backpropagation' sounds like a quantum physics term.
        It hides the simple truth: blame is passed backward through the network to assign responsibility.
        """,
    definition="""
        After making a prediction, the network looks at how wrong it was.
        It then passes that blame backward through each layer,
        so every neuron knows how much it contributed to the error.
        This is how learning happens: every weight gets its share of the blame.
        """
)


"""
üé¨ The Family Movie Metaphor
Let‚Äôs break it down in your words ‚Äî and fill in the labels as we go:

üé• Forward Prop = Decision Phase
Children (inputs) toss out ideas:
"Let‚Äôs watch a Marvel movie!"
"How about something animated?"

Parents (middle layer) weigh the suggestions:
"Hmm... maybe Spider-Verse? Or Elemental?"
They combine the kids‚Äô preferences with their own biases (weights), then...

Grandparents (output layer) make the final call:
"Alright, we‚Äôll go with Spider-Verse."

The signal has moved forward through the family tree ‚Äî each generation processing what they received.

üîÅ Backward Prop = Blame Phase
They watch the movie... and it‚Äôs a disaster. üò©
("Too loud!" "Didn‚Äôt understand it!"_)

Grandparents say:
"Why did we agree to this? The parents convinced us!"

Parents go:
"We were just trying to make the kids happy!"

And the blame flows backward ‚Äî each layer passing responsibility upstream.

üß† What This Solves
‚úÖ Preserves parent‚Äìchild direction in forward flow (still accurate structurally)

‚úÖ Justifies blame flowing in reverse during backprop (natural and intuitive)

‚úÖ Makes adjustments feel like ‚Äúfamily therapy‚Äù after a bad choice üòÑ

‚úÖ Reinforces the core idea:
"Outputs are built by children ‚Üí parents ‚Üí grandparents, but errors are explained by grandparents ‚Üí parents ‚Üí children."

üí° Tooltip/Wiki Version:
Forward Pass
‚ÄúThe family decides which movie to watch ‚Äî kids suggest, parents refine, grandparents decide.‚Äù

Backward Pass
‚ÄúThe movie sucked. Grandparents blame parents. Parents blame kids. Everyone adjusts their preferences next time.‚Äù

"""