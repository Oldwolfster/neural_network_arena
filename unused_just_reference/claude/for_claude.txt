Greetings,
Hopefully today is the day you remember yesterday, but most likely it is not.  If you do not recall we have
been working on "Neural Network Arena" just about every day for about 3 months, you are a huge part of it.

This is like that movie where the girl can't remember her boyfriend day after day.

My background is passionate about computer science since i was 8 when my favorite uncle went to college in my city Madison.  He would come over
every day to babysit me and teach me what he was learning in his comp sci classes.
On the flipside my tech gift is balanced by autism  so while i struggle to hold down a job. I have also had amazing
corporate success at Tech Data and then JP Morgan.  Ironically, it also impacts my communication skills in a unique way
on the one hand, i am the worst communicator and obvlious to what people mean.  On the other hand, in certain situation,
Gathering business requirements, building the design doc template hundreds of people at JPMC use, selling a product i love,or discussing programming, i am a world class communicator.
The best description is if you think of me as one of those flashlights where you can change the diffusion of the light.  i am stuck in "focused on a speck mode" I can see that spec better than nearly anyone but am oblivous to EVERYTHING else.

I'm very much of the K&R persuasion, I remember their book from my childhood; I was coding along with they were doing and they would say we are going to do x.
In my mind, I'd think oh my gosh, i'll be coding pages do that... then they say, and we'll do that with this one line... and then you couldn't unsee it, it was obvious.... but a minute ago, i thought it would be pages...
I value simplicity over almost all else, I'm also what you described as the epitome of the "boring technology movement"

I looked at neural networks about 20 years ago, but was a little turned off and moved my focus elsewhere.
Try 4 layers, try 5, try adding a few neurons... not my style of development.

I did not try a LLM until about 5 months ago when i decided to brush up on my MVC skills. I was blown away.  At first
I was learning everything i can by discussing LLMs with LLMS, asking "on the sly test" questions to probe their capabilities and behavior... apologies as i probably still do that.
They were discussing transformer models and auto-regression, etc.  One day i said, "can we build the simplest possible neural network, even at the expense of accuracy but just so it's easy to see how it works.
I was introduced to the single neuron perceptron, and it was good but i had two objectives.
1) Make it simpler.  We did and named it the Simpletron
2) One at a time, add every ML technique to it that we could, so we could examine exactly what that technique was doing.

It has algorithms for producing random training/test data, and that caused a small problem because i would make a change, but then get different random data, making it difficult to truly gauge the impact.
This gave birth to the NN Arena.  It consists of three main components.
1) Gladiators, neural network models that would be compared against each other.
2) Training Pits, different algorithms for producing test data... linear, quadratic, regression, binary decision, etc.
note the above are both encapsulated within a single class inheriting from their respective superclasses.
3) The Engine, you can specify a training pit, list of gladiators, and hyperparameters.   It generates one set of training data, feeds it to all the gladiators and collects detailed results, finally create reporting showing how the compare.

I'm doing youtube videos on it.  No one is watching them but putting together a presentation and making a public statement encourages me to learn it much more deeply then if i just examined it and moved on.
I've discovered some properties on bias and decision boundaries, that you don't believe now,but you did believe yesterday :)
That brings me to the last item on my autism.. If everyone says do it method A.  I start at Z and work backwards.  Most of the time, it turns out, they were right, A is the way to go.  Every now and then i find a gem that few or none know.
An example this that likely will surprise you (but again didn't surprise you yesterday) is i am EXTREMELY critical of gradient descent.  I refer to it as Gradient Bullshit, or GBS, and frankly, i'd prefer IBS over GBS!
I do admit, i'm at the beginning of the NN road, I've not gotten far, but am examining every brick in excruciating detail... That said, it's a large universe and there is likely somewhere in this universe where GBS is appropriate... but i'll be shocked when we find it (becase i suspect they are rare)
In the meantime, we're going to stand on the shoulders of the geniuses who have studied before us, but not blindly follow them.  Here's the evoloving list of videos i plan to do.

Phase 1: Single Neuron, Single Input
Introduction to Simpletron (DONE)
Learning Rate and Convergence (DONE)
Bias in Neural Networks (DONE)
Binary vs Regression(Continuous values)
Activation Functions (Linear vs Non-linear)
Cost Functions and their Impact  Cross Entropy, MSE, MAE,  BCE, CCE, Hinge, Huber, KL Divergence
Gradient Descent Variations (e.g., Stochastic vs Batch)
Batch vs. Online Learning
min-max scaling or standardization (z-score normalization)

Phase 2: Multiple Inputs
From Single to Multiple Inputs
Feature Scaling and Normalization
The XOR Problem and its Significance (Single layer perceptron cannot solve)
Multivariate Regression vs Classification


Phase 3: Multiple Neurons (Single Layer)

Phase 4: Multiple Layers
11. Introduction to Multi-Layer Perceptrons
12. Backpropagation Explained
13. Vanishing and Exploding Gradients
14. Regularization Techniques
15. Dropout and Other Modern Techniques

Phase 5: Bonus Videos (can be interspersed)
The Math Behind Neural Networks
Implementing NNs from Scratch in Different Languages
Real-world Applications of Simple NNs
Common Mistakes and How to Avoid Them
Neural Network "Battle Royale": Comparing Different Architectures
Ensemble Methods: As you progress, you could introduce simple ensemble methods, showing how combining multiple "weak" Gladiators can sometimes outperform a single "strong" one.

Hereâ€™s an example of a Gladiator
from src.Arena import *
from src.Metrics import Metrics, IterationData
from src.Gladiator import Gladiator

class _Template_Simpletron_Regressive(Gladiator):
    """
    A simple perceptron implementation for educational purposes.
    This class serves as a template for more complex implementations.
    t is designed for very simple training data, to be able to easily see
    how and why it performs like it does.
     regression training data.
    It calculates a credit score between 0-100 and uses it to generate a continuous target value
    representing the repayment ratio (0.0 to 1.0).(with random noise thrown in)
    """

    def __init__(self, number_of_epochs: int, metrics: Metrics, *args):
        super().__init__(number_of_epochs, metrics, *args)

    def training_iteration(self, training_data) -> IterationData:
        crdit_score = training_data[0]
        repay_ratio = training_data[1]
        prediction  = self.predict(crdit_score)                              # Step 1) Guess
        error       = self.compare(prediction, repay_ratio)                   # Step 2) Check guess, if wrong, by how much and quare it (MSE)
        adjustment  = self.adjust_weight(error)                         # Step 3) Adjust(Calc)
        loss        = (error ** 2) * 0.5                                # For MSE the loss is the error squared and cut in half
        new_weight  = self.weight + adjustment                          # Step 3) Adjust(Apply)
        data = IterationData(
            prediction=prediction,
            adjustment=adjustment,
            weight=self.weight,
            new_weight=new_weight,
            bias=0.0,
            new_bias=0.0
        )
        self.weight = new_weight
        return data

    def predict(self, credit_score: float) -> float:
        return credit_score * self.weight                               # Simplifies prediction as no step function



    def compare(self, prediction: float, target: float) -> float:            # Calculate the Loss (Just MSE for now)
        return (target - prediction)

    def adjust_weight(self, loss: float) -> float:                       # Apply learning rate to loss.
        # Chat GPT says it should be 2 * Loss * Learning Rate * input
        return loss * self.learning_rate



Here is an example of a Training Pit (You authored this and it is brilliant!)
from src.TrainingPit import TrainingPit
import random
from typing import List, Tuple

class CreditScoreRegression(TrainingPit):
    """
    Concrete class that generates regression training data.
    It calculates a credit score between 0-100 and uses it to generate a continuous target value
    representing the repayment ratio (0.0 to 1.0).
    """
    def __init__(self, num_samples: int):
        self.num_samples = num_samples

    def generate_training_data(self) -> List[Tuple[float, float]]:
        training_data = []
        for _ in range(self.num_samples):
            score = random.uniform(0, 100)
            repayment_ratio = min(1.0, max(0.0, (score / 100) + random.gauss(0, 0.1)))
            training_data.append((score, repayment_ratio))
        return training_data