Backpropogation is based on the calculus chain rule.... think of it as a chain of links...


Link 1, derivative of loss with respect to prediction.
how much did the prediction impact the loss.   ALL OF IT?


ğŸ”— Link 1: Loss Gradient (Anchor of the Chain)
ğŸ“Œ Formula:

âˆ‚
ğ¿
ğ‘œ
ğ‘ 
ğ‘ 
âˆ‚
ğ‘ƒ
ğ‘Ÿ
ğ‘’
ğ‘‘
ğ‘–
ğ‘
ğ‘¡
ğ‘–
ğ‘œ
ğ‘›
=
ğ‘‡
ğ‘
ğ‘Ÿ
ğ‘”
ğ‘’
ğ‘¡
âˆ’
ğ‘ƒ
ğ‘Ÿ
ğ‘’
ğ‘‘
ğ‘–
ğ‘
ğ‘¡
ğ‘–
ğ‘œ
ğ‘›
âˆ‚Prediction
âˆ‚Loss
â€‹
 =Targetâˆ’Prediction
ğŸ’¡ Interpretation:
This is the gradient of the loss function with respect to the modelâ€™s output.

In MSE, this is simply the error:
(
ğ‘‡
ğ‘
ğ‘Ÿ
ğ‘”
ğ‘’
ğ‘¡
âˆ’
ğ‘ƒ
ğ‘Ÿ
ğ‘’
ğ‘‘
ğ‘–
ğ‘
ğ‘¡
ğ‘–
ğ‘œ
ğ‘›
)
=
(
âˆ’
0.981
)
(Targetâˆ’Prediction)=(âˆ’0.981)
ğŸ”— Link 2: Activation Gradient (Sigmoid Derivative)
ğŸ“Œ Formula:

âˆ‚
ğ‘ƒ
ğ‘Ÿ
ğ‘’
ğ‘‘
ğ‘–
ğ‘
ğ‘¡
ğ‘–
ğ‘œ
ğ‘›
âˆ‚
ğ‘…
ğ‘
ğ‘¤
ğ‘†
ğ‘¢
ğ‘š
=
ğœ
(
ğ‘§
)
â‹…
(
1
âˆ’
ğœ
(
ğ‘§
)
)
âˆ‚RawSum
âˆ‚Prediction
â€‹
 =Ïƒ(z)â‹…(1âˆ’Ïƒ(z))
ğŸ’¡ Interpretation:
This tells us how much the raw sum (pre-activation) contributes to the prediction.

In this case, since the output neuron uses Sigmoid, we use its derivative:
0.981
â‹…
(
1
âˆ’
0.981
)
=
0.0183
0.981â‹…(1âˆ’0.981)=0.0183
ğŸ”— Link 3: Blame (Î´)
ğŸ“Œ Formula:

ğ›¿
=
âˆ‚
ğ¿
ğ‘œ
ğ‘ 
ğ‘ 
âˆ‚
ğ‘ƒ
ğ‘Ÿ
ğ‘’
ğ‘‘
ğ‘–
ğ‘
ğ‘¡
ğ‘–
ğ‘œ
ğ‘›
Ã—
âˆ‚
ğ‘ƒ
ğ‘Ÿ
ğ‘’
ğ‘‘
ğ‘–
ğ‘
ğ‘¡
ğ‘–
ğ‘œ
ğ‘›
âˆ‚
ğ‘…
ğ‘
ğ‘¤
ğ‘†
ğ‘¢
ğ‘š
Î´= 
âˆ‚Prediction
âˆ‚Loss
â€‹
 Ã— 
âˆ‚RawSum
âˆ‚Prediction
â€‹
 
ğŸ’¡ Interpretation:

This combines the loss gradient with the activation gradient, showing the total influence of error on the neuron.
(
âˆ’
0.981
)
Ã—
(
0.0183
)
=
âˆ’
0.0179
(âˆ’0.981)Ã—(0.0183)=âˆ’0.0179
ğŸ”— Link 4: Weight Gradients (Final Adjustments)
Each weight update follows:

Î”
ğ‘Š
=
learningÂ rate
Ã—
ğ›¿
Ã—
input
Î”W=learningÂ rateÃ—Î´Ã—input
ğŸ’¡ Interpretation:

We now distribute the error signal to each weight by multiplying it with the input value.
If W1 = 0.5, and input = 0.76, update rule is:
0.5
+
(
0.1
Ã—
âˆ’
0.0179
Ã—
0.76
)
=
0.4986
0.5+(0.1Ã—âˆ’0.0179Ã—0.76)=0.4986
(Similarly for W2)
Final Chain Recap
Step	Formula	Value in Example
Link 1: Loss Gradient	
âˆ‚
ğ¿
ğ‘œ
ğ‘ 
ğ‘ 
âˆ‚
ğ‘ƒ
ğ‘Ÿ
ğ‘’
ğ‘‘
ğ‘–
ğ‘
ğ‘¡
ğ‘–
ğ‘œ
ğ‘›
=
(
ğ‘‡
ğ‘
ğ‘Ÿ
ğ‘”
ğ‘’
ğ‘¡
âˆ’
ğ‘ƒ
ğ‘Ÿ
ğ‘’
ğ‘‘
ğ‘–
ğ‘
ğ‘¡
ğ‘–
ğ‘œ
ğ‘›
)
âˆ‚Prediction
âˆ‚Loss
â€‹
 =(Targetâˆ’Prediction)	
âˆ’
0.981
âˆ’0.981
Link 2: Activation Gradient	
ğœ
(
ğ‘§
)
â‹…
(
1
âˆ’
ğœ
(
ğ‘§
)
)
Ïƒ(z)â‹…(1âˆ’Ïƒ(z))	
0.0183
0.0183
Link 3: Blame (Î´)	
ğ›¿
=
LossÂ Gradient
Ã—
ActivationÂ Gradient
Î´=LossÂ GradientÃ—ActivationÂ Gradient	
âˆ’
0.0179
âˆ’0.0179
Link 4: Weight Updates	
ğ‘Š
=
ğ‘Š
+
ğœ‚
â‹…
ğ›¿
â‹…
Input
W=W+Î·â‹…Î´â‹…Input	
0.5
â†’
0.4986
0.5â†’0.4986


